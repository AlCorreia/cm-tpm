{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6594be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "repo_dir = os.path.dirname(os.getcwd())\n",
    "sys.path.append(repo_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ac459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.reproducibility import seed_everything\n",
    "from models.lo import fast_bins_lo, bins_lo\n",
    "from models.cm import ContinuousMixture\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.datasets import load_debd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "gpus = None if device == 'cpu' else 1\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632e372",
   "metadata": {},
   "source": [
    "### Specify the datasets to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6a9a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBD_DATASETS = [\n",
    "    'nltcs',\n",
    "    'msnbc',\n",
    "    'kdd',\n",
    "    'plants',\n",
    "    'baudio',\n",
    "    'jester',\n",
    "    'bnetflix',\n",
    "    'accidents',\n",
    "    'tretail',\n",
    "    'pumsb_star',\n",
    "    'dna',\n",
    "    'kosarek',\n",
    "    'msweb',\n",
    "    'book',\n",
    "    'tmovie',\n",
    "    'cwebkb',\n",
    "    'cr52',\n",
    "    'c20ng',\n",
    "    'bbc',\n",
    "    'ad',\n",
    "]\n",
    "print(DEBD_DATASETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb4965",
   "metadata": {},
   "source": [
    "### Specify the number of bins to use and number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbd151",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins_list = [2**7, 2**8, 2**9, 2**10]\n",
    "n_epochs = 150\n",
    "print(n_bins_list, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e258a620",
   "metadata": {},
   "source": [
    "## Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939fed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lls_dict(lls_dict):\n",
    "    for n_bins in lls_dict.keys():\n",
    "        avg_lls_per_run = [np.mean(ll) for ll in lls_dict[n_bins]]\n",
    "        avg_ll = np.mean(avg_lls_per_run)\n",
    "        std_ll = np.std(avg_lls_per_run)\n",
    "        print('Evaluating using ' + str(n_bins) + ' bins..')\n",
    "        print('AVG LL: %f ' % avg_ll + ' STD LL: %f ' % std_ll)\n",
    "        print('Latex string: %.2f$\\\\pm$%.2f' % (avg_ll, std_ll))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03dcfa7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you run OOM you can tweak n_chunks and batch_size\n",
    "n_chunks = None\n",
    "batch_size = 128\n",
    "cm_clt = True\n",
    "\n",
    "for dataset_name in DEBD_DATASETS:\n",
    "    \n",
    "    train, valid, test = load_debd(dataset_name)\n",
    "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test, batch_size=batch_size)\n",
    "    print('Evaluating ' + dataset_name + '..\\n')\n",
    "\n",
    "    test_lls_dict = {n_bins: [] for n_bins in n_bins_list}\n",
    "    \n",
    "    log_dir = repo_dir + ('/logs/debd/cm_clt/' if cm_clt else '/logs/debd/cm_fact/')\n",
    "    folder_tree = list(os.walk(log_dir + dataset_name))\n",
    "    for n_bins in n_bins_list:\n",
    "        for folder in folder_tree:\n",
    "            if 'checkpoints' in folder[0]:\n",
    "                for ckpt in folder[2]:\n",
    "                    \n",
    "                    model = ContinuousMixture.load_from_checkpoint(folder[0] + '/' + ckpt).to(device)\n",
    "                    model.n_chunks = n_chunks\n",
    "                    model.missing = False\n",
    "                    \n",
    "                    if 'best_model_valid' in ckpt:\n",
    "                        seed_everything(42)\n",
    "                        z, log_w = bins_lo(model, n_bins, train_loader, valid_loader, max_epochs=n_epochs, lr=1e-3, device=device)\n",
    "                        test_lls_dict[n_bins].append(\n",
    "                            model.eval_loader(test_loader, z, log_w, device=device).cpu().numpy())\n",
    "                        \n",
    "    evaluate_lls_dict(test_lls_dict)\n",
    "    print('\\nLO ended on ' + dataset_name + '\\n')\n",
    "    print('---------------------------------------------------------------------------\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
